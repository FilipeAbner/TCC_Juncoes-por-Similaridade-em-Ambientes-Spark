#!/usr/bin/env python3
"""
Teste Avan√ßado PySpark - An√°lise de Dados
Simula√ß√£o de an√°lise de vendas usando DataFrames
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as spark_sum, avg, count, when, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
import sys

def test_dataframe_operations():
    """Teste avan√ßado com opera√ß√µes de DataFrame"""
    
    print("=" * 70)
    print("TESTE AVAN√áADO - AN√ÅLISE DE DADOS DE VENDAS")
    print("=" * 70)
    
    # Criar SparkSession
    spark = SparkSession.builder \
        .appName("Analise de Vendas") \
        .master("spark://192.168.1.7:7077") \
        .config("spark.executor.memory", "1g") \
        .config("spark.executor.cores", "2") \
        .getOrCreate()
    
    print(f"\n‚úì Conectado ao cluster Spark")
    print(f"‚úì Vers√£o: {spark.version}")
    
    # Definir schema dos dados
    schema = StructType([
        StructField("id_venda", IntegerType(), False),
        StructField("produto", StringType(), False),
        StructField("categoria", StringType(), False),
        StructField("quantidade", IntegerType(), False),
        StructField("preco_unitario", DoubleType(), False),
        StructField("regiao", StringType(), False),
    ])
    
    # Dados de exemplo: vendas de uma loja
    dados_vendas = [
        (1, "Notebook", "Eletr√¥nicos", 2, 3500.00, "Sul"),
        (2, "Mouse", "Perif√©ricos", 5, 50.00, "Norte"),
        (3, "Teclado", "Perif√©ricos", 3, 150.00, "Sul"),
        (4, "Monitor", "Eletr√¥nicos", 1, 1200.00, "Centro"),
        (5, "Cadeira", "M√≥veis", 4, 800.00, "Norte"),
        (6, "Mesa", "M√≥veis", 2, 1500.00, "Sul"),
        (7, "Webcam", "Perif√©ricos", 3, 300.00, "Centro"),
        (8, "Headset", "Perif√©ricos", 6, 200.00, "Norte"),
        (9, "Notebook", "Eletr√¥nicos", 1, 4000.00, "Centro"),
        (10, "Impressora", "Eletr√¥nicos", 2, 900.00, "Sul"),
        (11, "Mouse", "Perif√©ricos", 10, 45.00, "Centro"),
        (12, "Cadeira", "M√≥veis", 3, 750.00, "Sul"),
        (13, "Monitor", "Eletr√¥nicos", 2, 1100.00, "Norte"),
        (14, "Mesa", "M√≥veis", 1, 1600.00, "Centro"),
        (15, "Teclado", "Perif√©ricos", 4, 140.00, "Norte"),
    ]
    
    # Criar DataFrame
    df_vendas = spark.createDataFrame(dados_vendas, schema)
    
    print("\n" + "-" * 70)
    print("üìä DADOS ORIGINAIS")
    print("-" * 70)
    df_vendas.show()
    
    # An√°lise 1: Calcular valor total de cada venda
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 1: Valor Total por Venda")
    print("-" * 70)
    
    df_com_total = df_vendas.withColumn(
        "valor_total", 
        col("quantidade") * col("preco_unitario")
    )
    df_com_total.select("id_venda", "produto", "quantidade", "preco_unitario", "valor_total").show()
    
    # An√°lise 2: Vendas por Categoria
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 2: Resumo por Categoria")
    print("-" * 70)
    
    vendas_por_categoria = df_com_total.groupBy("categoria").agg(
        spark_sum("valor_total").alias("faturamento_total"),
        spark_sum("quantidade").alias("qtd_total_vendida"),
        count("id_venda").alias("num_vendas"),
        avg("valor_total").alias("ticket_medio")
    ).orderBy(col("faturamento_total").desc())
    
    vendas_por_categoria.show()
    
    # An√°lise 3: Vendas por Regi√£o
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 3: Resumo por Regi√£o")
    print("-" * 70)
    
    vendas_por_regiao = df_com_total.groupBy("regiao").agg(
        spark_sum("valor_total").alias("faturamento_total"),
        count("id_venda").alias("num_vendas"),
        avg("valor_total").alias("ticket_medio")
    ).orderBy(col("faturamento_total").desc())
    
    vendas_por_regiao.show()
    
    # An√°lise 4: Top 5 Produtos
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 4: Top 5 Produtos Mais Vendidos (em valor)")
    print("-" * 70)
    
    top_produtos = df_com_total.groupBy("produto").agg(
        spark_sum("valor_total").alias("faturamento"),
        spark_sum("quantidade").alias("qtd_vendida")
    ).orderBy(col("faturamento").desc()).limit(5)
    
    top_produtos.show()
    
    # An√°lise 5: Classifica√ß√£o de vendas
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 5: Classifica√ß√£o de Vendas (Alto/M√©dio/Baixo Valor)")
    print("-" * 70)
    
    df_classificado = df_com_total.withColumn(
        "classificacao",
        when(col("valor_total") >= 3000, "Alto Valor")
        .when(col("valor_total") >= 1000, "M√©dio Valor")
        .otherwise("Baixo Valor")
    )
    
    df_classificado.select("id_venda", "produto", "valor_total", "classificacao").show()
    
    # An√°lise 6: Estat√≠sticas Gerais
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 6: Estat√≠sticas Gerais")
    print("-" * 70)
    
    from pyspark.sql.functions import max as spark_max, min as spark_min
    
    stats = df_com_total.agg(
        spark_sum("valor_total").alias("faturamento_total"),
        avg("valor_total").alias("ticket_medio"),
        spark_max("valor_total").alias("maior_venda"),
        spark_min("valor_total").alias("menor_venda"),
        count("id_venda").alias("total_vendas")
    )
    
    stats.show()
    
    # An√°lise 7: Join - Criar tabela de metas e comparar
    print("\n" + "-" * 70)
    print("üìä AN√ÅLISE 7: Compara√ß√£o com Metas por Regi√£o")
    print("-" * 70)
    
    # Criar DataFrame de metas
    dados_metas = [
        ("Norte", 8000.00),
        ("Sul", 12000.00),
        ("Centro", 10000.00),
    ]
    
    df_metas = spark.createDataFrame(dados_metas, ["regiao", "meta"])
    
    # Join com vendas por regi√£o
    resultado_vs_meta = vendas_por_regiao.join(df_metas, "regiao")
    
    resultado_vs_meta = resultado_vs_meta.withColumn(
        "percentual_atingido",
        (col("faturamento_total") / col("meta") * 100).cast("decimal(10,2)")
    ).withColumn(
        "status",
        when(col("percentual_atingido") >= 100, "‚úì Meta Atingida")
        .otherwise("‚úó Abaixo da Meta")
    )
    
    resultado_vs_meta.select("regiao", "faturamento_total", "meta", "percentual_atingido", "status").show()
    
    # Resumo Final
    print("\n" + "=" * 70)
    print("‚úÖ AN√ÅLISE CONCLU√çDA COM SUCESSO!")
    print("=" * 70)
    
    # Obter totalizadores
    total_row = stats.collect()[0]
    print(f"\nüìà RESUMO EXECUTIVO:")
    print(f"   ‚Ä¢ Faturamento Total: R$ {total_row['faturamento_total']:,.2f}")
    print(f"   ‚Ä¢ Ticket M√©dio: R$ {total_row['ticket_medio']:,.2f}")
    print(f"   ‚Ä¢ Maior Venda: R$ {total_row['maior_venda']:,.2f}")
    print(f"   ‚Ä¢ Menor Venda: R$ {total_row['menor_venda']:,.2f}")
    print(f"   ‚Ä¢ Total de Vendas: {total_row['total_vendas']}")
    
    # Encerrar
    spark.stop()
    print("\n‚úì SparkSession encerrada")
    print("\n" + "=" * 70 + "\n")

if __name__ == "__main__":
    try:
        test_dataframe_operations()
    except Exception as e:
        print(f"\n‚ùå ERRO: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

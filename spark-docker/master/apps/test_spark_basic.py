#!/usr/bin/env python3
"""
Teste B√°sico PySpark - Contagem de Palavras
Este script testa a conectividade com o cluster Spark
"""

from pyspark.sql import SparkSession
import sys
import os

def print_header(text):
    """Imprime cabe√ßalho formatado"""
    print("\n" + "="*60)
    print(text)
    print("="*60 + "\n")

def print_section(text):
    """Imprime se√ß√£o formatada"""
    print("\n" + "-"*60)
    print(text)
    print("-"*60)

def main():
    print_header("INICIANDO TESTE B√ÅSICO DO PYSPARK")
    
    # Detectar Master URL
    master_url = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')
    print(f"üîó Conectando ao Master: {master_url}\n")
    
    # Criar SparkSession
    spark = SparkSession.builder \
        .appName("Teste Basico PySpark") \
        .master(master_url) \
        .config("spark.executor.memory", "512m") \
        .config("spark.executor.cores", "1") \
        .config("spark.sql.shuffle.partitions", "4") \
        .getOrCreate()
    
    print(f"‚úì SparkSession criada com sucesso!")
    print(f"‚úì Master: {spark.sparkContext.master}")
    print(f"‚úì App Name: {spark.sparkContext.appName}")
    print(f"‚úì Spark Version: {spark.version}\n")
    
    # Teste 1: RDD b√°sico
    print_header("TESTE 1: Criando RDD e contando elementos")
    
    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    rdd = spark.sparkContext.parallelize(data, numSlices=4)
    count = rdd.count()
    soma = rdd.sum()
    
    print(f"‚úì RDD criado com {count} elementos")
    print(f"‚úì Soma dos elementos: {soma}")
    
    # Teste 2: Criar DataFrame
    print_section("TESTE 2: Criando DataFrame e realizando opera√ß√µes")
    
    data_df = [
        ("Alice", 34, "Engenheira"),
        ("Bob", 45, "M√©dico"),
        ("Charlie", 28, "Designer"),
        ("Diana", 32, "Professora"),
        ("Eve", 29, "Desenvolvedor")
    ]
    
    df = spark.createDataFrame(data_df, ["nome", "idade", "profissao"])
    
    print(f"‚úì DataFrame criado com {df.count()} registros")
    print("\nüìä Primeiras linhas do DataFrame:")
    df.show()
    
    print("\nüìä Schema do DataFrame:")
    df.printSchema()
    
    # Teste 3: Transforma√ß√µes e a√ß√µes
    print_section("TESTE 3: Aplicando transforma√ß√µes")
    
    # Filtrar pessoas com mais de 30 anos
    df_filtrado = df.filter(df.idade > 30)
    print(f"\n‚úì Pessoas com mais de 30 anos:")
    df_filtrado.show()
    
    # Calcular m√©dia de idade
    from pyspark.sql.functions import avg, max, min, count
    
    stats = df.agg(
        avg("idade").alias("idade_media"),
        max("idade").alias("idade_maxima"),
        min("idade").alias("idade_minima"),
        count("nome").alias("total_pessoas")
    )
    
    print("\nüìä Estat√≠sticas:")
    stats.show()
    
    # Teste 4: Contagem de palavras (cl√°ssico MapReduce)
    print("\n" + "-" * 60)
    print("TESTE 4: Word Count (exemplo cl√°ssico)")
    print("-" * 60)
    
    texto = [
        "Apache Spark √© um framework de processamento distribu√≠do",
        "Spark √© r√°pido e escal√°vel",
        "PySpark √© a API Python do Apache Spark",
        "Clusters Spark processam big data de forma eficiente"
    ]
    
    rdd_texto = spark.sparkContext.parallelize(texto)
    
    # MapReduce: dividir em palavras, contar e ordenar
    word_counts = rdd_texto \
        .flatMap(lambda linha: linha.lower().split()) \
        .map(lambda palavra: (palavra, 1)) \
        .reduceByKey(lambda a, b: a + b) \
        .sortBy(lambda x: x[1], ascending=False)
    
    print("\nüìä Top 10 palavras mais frequentes:")
    for palavra, count in word_counts.take(10):
        print(f"   {palavra}: {count}")
    
    # Teste 5: Opera√ß√µes em paralelo
    print("\n" + "-" * 60)
    print("TESTE 5: Processamento em paralelo")
    print("-" * 60)
    
    # Criar um RDD maior para testar paraleliza√ß√£o
    numeros = spark.sparkContext.parallelize(range(1, 1001), numSlices=10)
    
    # Aplicar transforma√ß√£o: elevar ao quadrado
    quadrados = numeros.map(lambda x: x ** 2)
    
    # Filtrar apenas n√∫meros pares
    pares = quadrados.filter(lambda x: x % 2 == 0)
    
    total_pares = pares.count()
    soma_pares = pares.sum()
    
    print(f"‚úì Total de quadrados pares: {total_pares}")
    print(f"‚úì Soma dos quadrados pares: {soma_pares}")
    print(f"‚úì N√∫mero de parti√ß√µes: {pares.getNumPartitions()}")
    
    # Informa√ß√µes finais
    print("\n" + "=" * 60)
    print("‚úÖ TODOS OS TESTES CONCLU√çDOS COM SUCESSO!")
    print("=" * 60)
    print(f"\nüìù Informa√ß√µes do Cluster:")
    print(f"   - Master: {spark.sparkContext.master}")
    print(f"   - Default Parallelism: {spark.sparkContext.defaultParallelism}")
    
    # Encerrar sess√£o
    spark.stop()
    print("\n‚úì SparkSession encerrada")
    print_header("FIM DOS TESTES")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå ERRO: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

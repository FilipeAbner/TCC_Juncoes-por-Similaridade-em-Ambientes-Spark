# ConfiguraÃ§Ã£o do Cluster Spark Multi-MÃ¡quina

Este guia explica como configurar um cluster Spark com Master e Workers em mÃ¡quinas diferentes utilizando docker.

## Topologia do Cluster em mÃ¡quinas diferentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MÃ¡quina 1     â”‚         â”‚   MÃ¡quina 2     â”‚         â”‚   MÃ¡quina 3     â”‚
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚  Spark Master   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Spark Worker   â”‚         â”‚  Spark Worker   â”‚
â”‚  IP: 192.168... â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚  Porta: 7077    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ConfiguraÃ§Ã£o do Master (MÃ¡quina 1)

### Passo 1: Descobrir o IP da mÃ¡quina Master

```bash
# Linux/Mac
ip addr show | grep inet

# Ou
hostname -I

```

Anote o IP (exemplo: `192.168.1.100`)

### Passo 2: Configurar .env para o Master e Worker

**ATENÃ‡ÃƒO:** Esta Ã© a configuraÃ§Ã£o mais crÃ­tica!

No arquivo `master/.env`, **SUBSTITUA** pelos valores reais:

```env
SPARK_MODE=master
SPARK_MASTER_HOST=0.0.0.0  # Sempre 0.0.0.0 para aceitar conexÃµes
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_DRIVER_HOST=192.168.1.100  # â† SEU IP REAL DA MÃQUINA MASTER
SPARK_DRIVER_PORT=35000
SPARK_DRIVER_BLOCKMANAGER_PORT=35100
```

**âš ï¸ IMPORTANTE:** As variÃ¡veis `SPARK_DRIVER_*` sÃ£o automaticamente substituÃ­das no `spark-defaults.conf` durante a inicializaÃ§Ã£o do container.

No arquivo `worker/.env`, use o **IP REAL do Master**:

```env
SPARK_MODE=worker
SPARK_MASTER_URL=spark://192.168.1.100:7077  # â† IP REAL DO MASTER
SPARK_MASTER_HOST=192.168.1.100  # â† MESMO IP
SPARK_WORKER_WEBUI_PORT=8081
```


### Passo 3: Verificar o Firewall

**IMPORTANTE:** As portas precisam estar abertas no firewall do Master!

```bash
# Ubuntu/Debian
sudo ufw allow 7077/tcp    # Master RPC
sudo ufw allow 8080/tcp    # Master UI
sudo ufw allow 35000/tcp   # Driver (comunicaÃ§Ã£o com executores)

# CentOS/RHEL
sudo firewall-cmd --permanent --add-port=7077/tcp
sudo firewall-cmd --permanent --add-port=8080/tcp
sudo firewall-cmd --permanent --add-port=35000/tcp
sudo firewall-cmd --reload
```

### Passo 4: Iniciar o Master

```bash
cd master
sudo docker compose up --build
```

Verifique se estÃ¡ rodando:
- Web UI: `http://IP_DO_MASTER:8080`
- Porta RPC: `IP_DO_MASTER:7077`

### Passo 5: Configurar NFS para Compartilhamento de Arquivos (Recomendado)

**IMPORTANTE:** Para que os Workers acessem os mesmos arquivos que o Master, vamos exportar o diretÃ³rio `apps` via NFS.

 Criar network docker externa:
```bash 
    sudo docker network create spark-network --driver bridge --subnet 172.20.0.0/16 --gateway 172.20.0.1
```


#### **MÃ©todo Recomendado: Exportar diretÃ³rio `apps` do Master**

Este mÃ©todo garante que Master e Workers vejam exatamente os mesmos arquivos.

```bash
# 1. Parar Master (se estiver rodando)
cd ~/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master
sudo docker-compose down

# 2. Instalar servidor NFS
sudo apt-get update
sudo apt-get install -y nfs-kernel-server

# 3. Obter caminho absoluto do diretÃ³rio apps
APPS_DIR=$(pwd)/apps
echo "Exportando: $APPS_DIR"

# 4. Configurar NFS para exportar apps (substitua pelo caminho real)
# Exemplo: /home/filipe-abner/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps
echo "$APPS_DIR *(rw,sync,no_subtree_check,no_root_squash,insecure,all_squash,anonuid=1000,anongid=1000)" | sudo tee -a /etc/exports

# 5. Aplicar configuraÃ§Ãµes
sudo exportfs -ra
sudo systemctl restart nfs-kernel-server

# 6. Verificar se estÃ¡ exportando
sudo exportfs -v | grep apps

# 7. Liberar firewall (se ativo)
sudo ufw allow from 192.168.1.0/24 to any port nfs
sudo ufw allow from 192.168.1.0/24 to any port 2049

# 8. Subir Master novamente
sudo docker-compose up -d
```

**Verificar no Master:**
```bash
# Listar arquivos compartilhados
ls -la ~/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps/
# Deve mostrar: test_nfs_data.py, vendas_exemplo.csv, etc.
```

---

## ConfiguraÃ§Ã£o do Worker (MÃ¡quina 2, 3, ...)

#### **No Worker Linux:**

```bash
# 1. Instalar cliente NFS
sudo apt-get update
sudo apt-get install -y nfs-common

# 2. Criar ponto de montagem
sudo mkdir -p /mnt/master-apps

# 3. Testar se consegue ver o compartilhamento NFS do Master
showmount -e 192.168.1.6
# Deve mostrar: /home/filipe-abner/.../master/apps *

# 4. Montar o diretÃ³rio apps do Master
# SUBSTITUA o caminho pelo caminho real retornado por showmount
sudo mount -t nfs 192.168.1.6:/home/filipe-abner/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps /mnt/master-apps

# 5. Verificar se montou corretamente
ls -la /mnt/master-apps/
# Deve mostrar: test_nfs_data.py, vendas_exemplo.csv, etc.

# 6. Testar acesso
cat /mnt/master-apps/vendas_exemplo.csv | head -5

# 7. (Opcional) Tornar montagem permanente apÃ³s reboot
echo "192.168.1.7:/home/filipe-abner/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps /mnt/master-apps nfs defaults,_netdev 0 0" | sudo tee -a /etc/fstab
```

#### **No Worker Windows com WSL:**

```bash
# 1. Instalar cliente NFS
sudo apt-get update
sudo apt-get install -y nfs-common

# 2. Criar ponto de montagem
sudo mkdir -p /mnt/master-apps

# 3. Testar se o NFS estÃ¡ acessÃ­vel
showmount -e 192.168.1.7

# 4. Montar o diretÃ³rio apps do Master (substitua pelo caminho real)
sudo mount -t nfs 192.168.1.7:/home/filipe-abner/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps /mnt/master-apps

# 5. Verificar montagem
ls -la /mnt/master-apps/

# 6. Se der erro "access denied", tente com opÃ§Ãµes adicionais:
sudo mount -t nfs -o vers=3,proto=tcp 192.168.1.7:/home/filipe-abner/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps /mnt/master-apps
```

**âš ï¸ Troubleshooting - Se der erro "access denied":**

1. **Verificar no Master** se o diretÃ³rio apps estÃ¡ sendo exportado:
   ```bash
   # No Master
   sudo exportfs -v | grep apps
   # Deve mostrar o caminho completo do diretÃ³rio apps
   ```

2. **Recarregar configuraÃ§Ã£o do NFS** no Master:
   ```bash
   sudo exportfs -ra
   sudo systemctl restart nfs-kernel-server
   ```

3. **Verificar conectividade de rede**:
   ```bash
   # No Worker
   ping 192.168.1.7
   nc -zv 192.168.1.7 2049
   ```

#### **Adicionar volume ao docker-compose do Worker:**

Edite `worker/docker-compose.yml` e adicione o volume:
```yaml
volumes:
  - /mnt/master-apps:/apps  # â† Monta o diretÃ³rio apps do Master
```

Agora Master e Workers compartilham o mesmo diretÃ³rio `/apps` com todos os arquivos!
volumes:
  - /shared/spark-data:/data  # â† Mesmo volume NFS do Master
```

Agora todos os Workers podem acessar os mesmos arquivos em `/data`.

**Exemplo de uso:**
```python
# No cÃ³digo PySpark
df = spark.read.csv("/data/vendas.csv", header=True)
# Todos os executors conseguem acessar o arquivo!
```

### Passo 2: Testar conectividade com o Master

Antes de subir o worker, teste se consegue alcanÃ§ar o Master:

```bash
# Teste de ping
ping 192.168.1.100

# Teste de porta (deve retornar algo, nÃ£o "Connection refused")
telnet 192.168.1.100 7077
# ou
nc -zv 192.168.1.100 7077
```

### Passo 3: Iniciar o Worker

```bash
cd worker
sudo docker compose up --build
```

### Passo 4: Verificar se o Worker conectou

Veja os logs do worker:
```bash
sudo docker logs -f spark-worker
```

Se conectou com sucesso, vocÃª verÃ¡ algo como:
```
INFO Worker: Successfully registered with master spark://192.168.1.100:7077
```

Verifique tambÃ©m na Web UI do Master (`http://IP_DO_MASTER:8080`), o worker deve aparecer na lista.

### Passo 5: Testar com dados compartilhados via NFS

Execute o teste de leitura de dados compartilhados:

```bash
# Na mÃ¡quina do Master
sudo docker exec -it spark-master \
  spark-submit \
  --master spark://192.168.1.7:7077 \
  --executor-memory 512m \
  --executor-cores 1 \
  --total-executor-cores 2 \
  /apps/test_nfs_data.py
```

**Resultado esperado:**
```
======================================================================
TESTE: Leitura de Dados do NFS Compartilhado
======================================================================

âœ“ Arquivo lido com sucesso!
âœ“ Total de registros: 60
âœ“ AnÃ¡lises estatÃ­sticas executadas
âœ“ Resultado salvo em: /apps/resultado_analise_vendas
âœ… TESTE CONCLUÃDO COM SUCESSO!
```

**Verificar resultado:**
```bash
# Verificar arquivos gerados
ls -la ~/TCC_Juncoes-por-Similaridade-em-Ambientes-Spark/spark-docker/master/apps/resultado_analise_vendas/

# Deve mostrar:
# _SUCCESS
# part-xxxxx.snappy.parquet
```

### Passo 6: Teste bÃ¡sico de conectividade

Execute um teste simples sem dados externos:
```bash
# Na mÃ¡quina do Master
sudo docker exec -it spark-master \
  spark-submit \
  --master spark://192.168.1.7:7077 \
  --executor-memory 512m \
  --executor-cores 1 \
  --total-executor-cores 2 \
  /apps/test_spark_basic.py
```

**âš ï¸ ATENÃ‡ÃƒO:** Use o IP real do Master no `--master`, nÃ£o o hostname `spark-master`!

**âœ… ValidaÃ§Ã£o do Compartilhamento NFS:**

Este teste comprova que:
- âœ… Master e Workers acessam o mesmo diretÃ³rio `/apps`
- âœ… Arquivos sÃ£o compartilhados via NFS
- âœ… Executors conseguem ler e processar dados compartilhados
- âœ… Resultados sÃ£o salvos no diretÃ³rio compartilhado
- âœ… Processamento distribuÃ­do funciona corretamente

Se os executores nÃ£o conectarem, verifique:
1. `SPARK_MASTER_HOST` no Master estÃ¡ com o IP real (nÃ£o `0.0.0.0`)
2. Firewall permite trÃ¡fego bidirecional na porta 7077
3. Workers conseguem fazer ping no IP do Master

---

## ConfiguraÃ§Ã£o Adicionais

### Limitar recursos do Worker

Edite `worker/.env`:

```env
SPARK_WORKER_CORES=4        # NÃºmero de CPUs para o Worker usar
SPARK_WORKER_MEMORY=8g      # MemÃ³ria RAM para o Worker
SPARK_WORKER_PORT=7078      # Porta de comunicaÃ§Ã£o do Worker
```

## Topologia do cluster na mesma mÃ¡quina

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
â”‚   MÃ¡quina 1     â”‚
â”‚                 â”‚  
â”‚  Spark Master   â”‚
â”‚  Porta: 7077    â”‚
â”‚   Spark Worker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           
```

## ConfiguraÃ§Ã£o do Master e Worker na mesma mÃ¡quina

#### 1. Criar rede Docker externa
```bash
sudo docker network create spark-network \
  --driver bridge \
  --subnet 172.20.0.0/16 \
  --gateway 172.20.0.1
```

#### 2. Configurar .env para mesma mÃ¡quina

No arquivo `master/.env`:
```env
SPARK_MODE=master
SPARK_MASTER_HOST=0.0.0.0
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_DRIVER_HOST=192.168.1.7  # Use seu IP local (nÃ£o afeta mesma mÃ¡quina)
SPARK_DRIVER_PORT=35000
SPARK_DRIVER_BLOCKMANAGER_PORT=35100
```

No arquivo `worker/.env`:
```env
SPARK_MODE=worker
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_MASTER_HOST=spark-master
SPARK_WORKER_WEBUI_PORT=8081
```

**Importante:** Para mesma mÃ¡quina, o Worker usa o hostname `spark-master` que Ã© resolvido automaticamente pelo Docker.

#### 3. Iniciar os containers
```bash
# Master
cd master
sudo docker compose up --build -d

# Worker (em outro terminal)
cd worker
sudo docker compose up --build -d
```


### Executar o Script

#### **OpÃ§Ã£o A: Via spark-submit no Master (Recomendado)**

```bash
# Copiar script para o Master (se ainda nÃ£o estiver)
sudo docker cp master/apps/test_spark_basic.py spark-master:/apps/test_spark_basic.py

# Executar com spark-submit
sudo docker exec -it spark-master \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 512m \
  --executor-cores 1 \
  --total-executor-cores 2 \
  /apps/test_spark_basic.py
```

#### **OpÃ§Ã£o B: Via container interativo**

```bash
# Entrar no container Master
sudo docker exec -it spark-master /bin/bash

# Dentro do container, executar:
spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 512m \
  --executor-cores 1 \
  /apps/test_spark_basic.py
```

## ğŸ“Š Consultar Logs Completos

### Ver logs em tempo real:
```bash
sudo docker exec -it spark-master tail -f /opt/spark/logs/$(sudo docker exec spark-master ls -t /opt/spark/logs/ | grep spark- | head -1)
```

### Spark Events

Os logs de eventos do Spark sÃ£o salvos em `/opt/spark/events` dentro do container Master. Junto da inicializaÃ§Ã£o do master 
os eventos tambem sÃ£o montados em um volume local `./master/spark-events` para facilitar o acesso, e analisar o ciclo da aplicaÃ§Ã£o sendo possivel acessa-los diretamente na mÃ¡quina host atravÃ©s da url:

```bash
  http://localhost:18080
```